{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600186794551",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añade al archivo \"3.regression_classification_exercise\" del CW8D4/exercises, todos los algoritmos que hemos visto. Estos son:\n",
    "\n",
    "### Regresión:\n",
    "\n",
    "- Linear Regression - D\n",
    "- SVM (versión regresión SVR) - D\n",
    "- Polinominal Regression - D\n",
    "- Random Forest (versión regresión) -D\n",
    "\n",
    "### Clasificación:\n",
    "\n",
    "- SVM (versión regresión SVC) - D\n",
    "- Knn - D\n",
    "- Random Forest (versión clasificación) - D\n",
    "- Xgboost (si todo OK) - D\n",
    "- Logistic regression - D\n",
    "\n",
    "Haz que se puedan ejecutar de forma genérica para varias features de los algoritmos. Por ejemplo, que se ejecute con \"param\" para diferentes grados del polinomio y para usar diferentes kernels en SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model(option_user, parameters):\n",
    "    if option_user == \"Regression\":\n",
    "        model_option = input(\"Which model do you want to use (1,2,3,4)?\")\n",
    "        option_user = int(model_option)\n",
    "        if option_user == 1:\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            if parameters.keys():\n",
    "                for k, v in parameters.items():\n",
    "                    model = LinearRegression(**v) \n",
    "                return model\n",
    "            else:\n",
    "                model = LinearRegression()\n",
    "                return model\n",
    "\n",
    "        elif option_user == 2:\n",
    "            from sklearn.preprocessing import PolynomialFeatures\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            if \"degree\" in parameters.keys():\n",
    "                degree = params[\"degree\"]\n",
    "                model = PolynomialFeatures(degree) \n",
    "                return model\n",
    "            else:\n",
    "                print(\"Missing argument degree\")\n",
    "        elif option_user == 3:\n",
    "            from sklearn.svm import SVR\n",
    "            model = SVR()\n",
    "            return model  \n",
    "        elif option_user == 4:\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            model = RandomForestRegressor()\n",
    "            return model\n",
    "\n",
    "        return model \n",
    "        \n",
    "    elif option_user == \"classification\":\n",
    "        model_option = input(\"Which model do you want to use (1,2,3,4,5)?\")\n",
    "        option_user = model_option\n",
    "\n",
    "        if option_user == 1:\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            model = LogisticRegression()\n",
    "            return model\n",
    "        if option_user ==2:\n",
    "            from sklearn import svm\n",
    "            model  = svm.SVC()\n",
    "            return model\n",
    "        if option_user == 3:\n",
    "            from sklearn.neighbors import KNeighborsClassifier\n",
    "            if \"K\" in parameters.keys():\n",
    "                K = params[\"K\"]\n",
    "                model = KNeighborsClassifier(n_neighbors=K)\n",
    "                return model\n",
    "            else:\n",
    "                print(\"Missing argument K\")\n",
    "        if option_user == 4:\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            model  = RandomForestClassifier()\n",
    "            return model\n",
    "        if option_user == 5:\n",
    "            from xgboost import XGBClassifier\n",
    "            model  = XGBClassifier()\n",
    "            return model\n",
    "\n",
    "        return model\n",
    "\n",
    "def train_model(model, df, target_name):\n",
    "    X = df.drop(target_name, 1)\n",
    "    y = df[target_name]\n",
    "    \n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    \n",
    "    \n",
    "    if str(model).startswith(\"PolynomialFeatures\"):\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        polyreg=make_pipeline(model,LinearRegression())\n",
    "        model_trained = polyreg.fit(X,y)  # function gives error with X_Train, y_train, why?\n",
    "        accuracy = model_trained.score(X_test, y_test)\n",
    "\n",
    "    else:\n",
    "        model_trained = model.fit(X_train, y_train)\n",
    "        accuracy = model_trained.score(X_test, y_test)\n",
    "\n",
    "    return model_trained, accuracy\n",
    "\n",
    "'''\n",
    "for regression: \n",
    "    option 1 = LinearRegression\n",
    "    option 2 = PolynomialFeatures\n",
    "    option 3 = SVM - SVR\n",
    "    option 4 = RandomForestRegressor\n",
    "for classification: \n",
    "    option 1 = LogisticRegression\n",
    "    option 2 = KNeighborsClassifier\n",
    "    option 3 = svm - SVC\n",
    "    option 4 = RandomForestClassifier()\n",
    "    option 5 = XGBClassifier()\n",
    "\n",
    "paremeters should be dictionary\n",
    "'''\n",
    "def main(df, parameters=None):\n",
    "    choice = input(\"What type of problem: regression or classifaction?\")\n",
    "    #params = input(\"Put in any parameters if necessary like: parameter=value, if not neccesary put no.\") \n",
    "    target = input(\"What is the target column?\")\n",
    "    if not parameters:\n",
    "        model = choose_model(option_user=choice)\n",
    "        model_trained, accuracy = train_model(model=model, df=df, target_name=target)\n",
    "    else:\n",
    "        model = choose_model(choice, parameters)\n",
    "        model_trained, accuracy = train_model(model=model, df=df, target_name=target)\n",
    "\n",
    "    print(accuracy)\n",
    "    return model_trained\n",
    "\n",
    "def predict_new_data(model_trained, to_pred):\n",
    "    if not isinstance(to_pred,(np.ndarray)):\n",
    "        raise TypeError('wrong type, to_pred must be a numpy array list')\n",
    "    else:\n",
    "        try:\n",
    "            y_pred = model_trained.predict(to_pred)\n",
    "            return y_pred\n",
    "        except:\n",
    "            to_pred = to_pred.reshape(1, -1)\n",
    "            y_pred = model_trained.predict(to_pred)\n",
    "            return y_pred\n",
    " \n",
    "  "
   ]
  },
  {
   "source": [
    "testing parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"usuarios_win_mac_lin.csv\")\n",
    "parameters = {\"normalize\" : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "ABCMeta object argument after ** must be a mapping, not int",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-008be178e39e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-117-664c57dea777>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(df, parameters)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mmodel_trained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchoose_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[0mmodel_trained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-117-664c57dea777>\u001b[0m in \u001b[0;36mchoose_model\u001b[1;34m(option_user, parameters)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ABCMeta object argument after ** must be a mapping, not int"
     ]
    }
   ],
   "source": [
    "main(df=df1, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'params': 'normalize=True'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "normalize=True\n"
    }
   ],
   "source": [
    "for elem in dic.values():\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}