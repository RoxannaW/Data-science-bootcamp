{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600351255508",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-071a55f608b7>, line 12)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-071a55f608b7>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    bv -> from Utils.ML_creating_training_models import main\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "root_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('')))))\n",
    "sys.path.append(root_path)\n",
    "\n",
    "sys.path.append(\"C:\\\\Users\\\\Roxan\\\\OneDrive\\\\Documentos\\\\My_map_2\\\\Data-science-bootcamp\")\n",
    "\n",
    "bv -> from Utils.ML_creating_training_models import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import plot\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedKFold, KFold\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "from Utils import my_functions\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list of highly correlated columns. \n",
    "indices = np.where(df_corr > 0.5)\n",
    "indices = [(df_corr.columns[x], df_corr.columns[y]) for x, y in zip(*indices) if x != y and x < y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding correlated columns with target column\n",
    "cor_target = abs(corr[\"product_name\"])\n",
    "#relevant_features = cor_target[cor_target>0.3]\n",
    "cor_target.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df.isnull().sum()\n",
    "print(\"Number of null values in each column:\\n{}\".format(null_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data types and their frequency\\n{}\".format(df.dtypes.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the object columns specifically\n",
    "object_columns_df = df.select_dtypes(include=['object'])\n",
    "print(object_columns_df.iloc[0])\n",
    "\n",
    "\n",
    "cols = ['2', '4', '6', '7', '8', '12', '13', '15', '17']\n",
    "for name in cols:\n",
    "    print(name,':')\n",
    "    print(object_columns_df[name].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting columns with specific caracters (in string) to then change. \n",
    "col_to_change = [col for col in df.columns if df[col].astype(str).str.contains('%').any()]\n",
    "for elem in col_to_change:\n",
    "    print(elem)\n",
    "    df[elem] = df[elem].apply(lambda x: float(str(x)[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To add dataframes to each other\n",
    "df = pd.concat([X_others, X_categorical_no_numbers], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" To read all line of a json file. \"\"\"\n",
    "#with open('name_file') as f:\n",
    "#    line = f.readline()\n",
    "#    if line:\n",
    "#        print(line)\n",
    "#        line = f.readline()\n",
    "# print(line)\n",
    "\n",
    "\n",
    "\"\"\" Lambda function to transform to float and not take into account first elemen (dollar sign in most\n",
    " cases,but change how needed.\"\"\"\n",
    "\n",
    "Floater = lambda x: float(x[1:-1])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"function to change gender column to numeric, to then use to calculate ratios\"\"\"\n",
    "def gender_to_numeric(x):\n",
    "    if x == 'M':\n",
    "        return 1\n",
    "    if x == 'F':\n",
    "        return 0\n",
    "\n",
    "# apply the function to the gender column and create a new column\n",
    "df['new_column_name'] = df['gender_column'].apply(gender_to_numeric)\n",
    "# use the new column and sum via groupby specific column and the vulue count of the specific column to calculate ratio\n",
    "variabel = (df.groupby('specific_column').new_column_name.sum() / df.specific_column.value_counts()) * 100 \n",
    "# add % sign\n",
    "variabel.astype(str) + \"%\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" lambda function to show all the columns and values after doing a groupby \"\"\"\n",
    "lambda df:df.sort_values(by=[\"column_name1\",\"column_name2\"]) # can also do only one column\n",
    "\n",
    "#use as following: \n",
    "#variabel = name_df.groupby([\"column_name1\", \"column_name2\"])\n",
    "#variabel = variabel.apply(\"insert Lambda function\")\n",
    "#delete the duplicate columns names\n",
    "\n",
    "\n",
    "\"\"\"function to replace values of a column that are negative with the mean value \n",
    "    of the column (not taking into account the negative values)\"\"\"\n",
    "\n",
    "def replace(group):\n",
    "    mask = group<0\n",
    "    group[mask] = group[~mask].mean()\n",
    "    return group\n",
    "\n",
    "## use as following: new_info = df.groupby(\"columns name of groupby\")[\"column to take into account\"].apply(replace)\n",
    "\n",
    "\n",
    "\"\"\"Clean strings:\"\"\"\n",
    "df[\"Column\"].str.elem.extract('([a-zA-Z\\s]+)', expand=False).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polinominal\n",
    "polinominal_model = PolynomialFeatures(degree=3) \n",
    "X_poly = polinominal_model.fit_transform(X_train, y_train)\n",
    "\n",
    "lin_reg_model = LinearRegression()\n",
    "lin_reg_model.fit(X_poly, y_train)\n",
    "\n",
    "y_pred = lin_reg_model.predict(X_poly)\n",
    "\n",
    "plt.xlim(20,55)\n",
    "plt.ylim(0,12)\n",
    "\n",
    "X_train_to_show, y_train_to_show = zip(*sorted(zip(X_train, y_train)))\n",
    "plt.scatter(X, y, color=\"green\")\n",
    "X_train_to_show, y_pred = zip(*sorted(zip(X_train, y_pred)))\n",
    "plt.plot(X_train_to_show, y_pred, color=\"blue\")\n",
    "\n",
    "\n",
    "plt.title(\"polynominal regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True # Si queremos que muestre más información durante el entrenamiento\n",
    "\n",
    "linear_params = {\n",
    "    'classifier': [LinearRegression()]\n",
    "    #'classifier__normalizebool': [True, False],\n",
    "    #'classifier__fit_intercept': [True, False]\n",
    "    }\n",
    "svr_params = {\n",
    "    'classifier': [ SVR()],\n",
    "    'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "    'classifier__degree': [1, 3, 5],\n",
    "    'classifier__gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "search_space = [\n",
    "    svr_params\n",
    "    #linear_params\n",
    "    #logistic_params,\n",
    "    #random_forest_params,\n",
    "    #svm_params,\n",
    "    #xgboost_params,\n",
    "    #knn_params,\n",
    "    ]\n",
    "\n",
    "# Le podemos poner cualquier clasificador. Irá cambiando según va probando pero necesita 1.\n",
    "# Si solo vamos a usar uno, debemos poner aquí que vamos a usar ese.\n",
    "pipe = Pipeline(steps=[('classifier',  svr_params[\"classifier\"][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model_filename' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bd97e82e3855>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodel_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_save\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_filename' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(file=model_filename, mode=\"rb\") as model_save:\n",
    "    model = pickle.load(model_save)\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}