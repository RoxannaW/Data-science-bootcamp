{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600890777097",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "1. Realizar una reducción de dimensionalidad/PCA para intentar aumentar el porcentaje de acierto. ¿Aumenta? ¿Cómo cambia el acierto según diferentes valores de PCA? Prueba, como mínimo, con valores para \"n_components\" 2,4,5 y 0.45 y 0.95. ¿Ha habido algún problema con algún valor?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for this exercise I used my pca_df function in Utils. for each n_components I tried, I saved the score of the test set. Then I created a df in which we can compare the scores of the different models with the differen n_components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               pca_2  pca_4  pca_5  pca_0.45  pca_0.95\nmodel                                                 \ndecision_tree  83.80  83.24  82.68     82.68     83.80\nrandom_forest  83.80  83.24  84.36     82.68     83.80\nSVC            73.74  82.12  80.45     65.36     73.74\nlogreg         73.74  81.01  80.45     65.36     73.74\nLinearSVC      73.74  81.01  79.89     67.60     73.74\nknn            79.89  80.45  79.33     79.33     79.89\nperceptron     67.60  79.89  77.09     48.04     67.60\nsgdclassifier  74.86  74.30  81.56     48.04     73.18\ngaussian       36.31  67.04  69.27     36.31     36.31",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pca_2</th>\n      <th>pca_4</th>\n      <th>pca_5</th>\n      <th>pca_0.45</th>\n      <th>pca_0.95</th>\n    </tr>\n    <tr>\n      <th>model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>decision_tree</th>\n      <td>83.80</td>\n      <td>83.24</td>\n      <td>82.68</td>\n      <td>82.68</td>\n      <td>83.80</td>\n    </tr>\n    <tr>\n      <th>random_forest</th>\n      <td>83.80</td>\n      <td>83.24</td>\n      <td>84.36</td>\n      <td>82.68</td>\n      <td>83.80</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>73.74</td>\n      <td>82.12</td>\n      <td>80.45</td>\n      <td>65.36</td>\n      <td>73.74</td>\n    </tr>\n    <tr>\n      <th>logreg</th>\n      <td>73.74</td>\n      <td>81.01</td>\n      <td>80.45</td>\n      <td>65.36</td>\n      <td>73.74</td>\n    </tr>\n    <tr>\n      <th>LinearSVC</th>\n      <td>73.74</td>\n      <td>81.01</td>\n      <td>79.89</td>\n      <td>67.60</td>\n      <td>73.74</td>\n    </tr>\n    <tr>\n      <th>knn</th>\n      <td>79.89</td>\n      <td>80.45</td>\n      <td>79.33</td>\n      <td>79.33</td>\n      <td>79.89</td>\n    </tr>\n    <tr>\n      <th>perceptron</th>\n      <td>67.60</td>\n      <td>79.89</td>\n      <td>77.09</td>\n      <td>48.04</td>\n      <td>67.60</td>\n    </tr>\n    <tr>\n      <th>sgdclassifier</th>\n      <td>74.86</td>\n      <td>74.30</td>\n      <td>81.56</td>\n      <td>48.04</td>\n      <td>73.18</td>\n    </tr>\n    <tr>\n      <th>gaussian</th>\n      <td>36.31</td>\n      <td>67.04</td>\n      <td>69.27</td>\n      <td>36.31</td>\n      <td>36.31</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "\n",
    "models = [\"logreg\", \"SVC\", \"knn\", \"gaussian\", \"perceptron\", \"LinearSVC\", \"sgdclassifier\", \"decision_tree\", \"random_forest\"]\n",
    "\n",
    "\n",
    "data = {\"pca_2\":[73.74, 73.74, 79.89, 36.31, 67.6, 73.74, 74.86, 83.8, 83.8], \"pca_4\": [81.01, 82.12, 80.45, 67.04, 79.89, 81.01, 74.3, 83.24, 83.24], \"pca_5\": [80.45, 80.45, 79.33, 69.27, 77.09, 79.89, 81.56, 82.68, 84.36],  \"pca_0.45\": [65.36, 65.36, 79.33, 36.31, 48.04, 67.6, 48.04, 82.68, 82.68], \"pca_0.95\": [73.74, 73.74, 79.89, 36.31, 67.6, 73.74, 73.18, 83.8, 83.8]}\n",
    "\n",
    "df_scores = pd.DataFrame(data, index = models)\n",
    "df_scores.index.name = 'model'\n",
    "\n",
    "df_scores.sort_values(by='pca_4', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "score mean of pca_2 is:  71.94222222222223\nscore mean of pca_4 is:  79.14444444444445\nscore mean of pca_5 is:  79.45333333333333\nscore mean of pca_0.45 is:  63.933333333333344\nscore mean of pca_0.95 is:  71.75555555555555\n"
    }
   ],
   "source": [
    "for elem in df_scores.columns:\n",
    "    mean = df_scores[elem].mean()\n",
    "    print(\"score mean of\", elem, \"is: \", mean)"
   ]
  },
  {
   "source": [
    "The reduction of the dimensions did not increase the accuracy score. The highest score with the original settings was:\t86.76 using Random Forest. The highest score we got with pca was 84.36 with Random Forest with n_components 5. \n",
    "\n",
    "In general the best scores (mean of scores of all models) were with a pca n_components of 5. The lowest scores were with n_components of 0.45. \n",
    "\n",
    "-> to ask: why not highest score with pca_0.95? done correctly?  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2. Realizar una normalización de los datos no categóricos y comprobar si aumenta los scores."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## used function --> normalize_dataframe(df=test_df, column=\"Fare\") from Utils.cleaning_function to normalize the columns: Fare. This is the only numeric column in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               Score\nknn            85.47\nSVC            84.36\nrandom_forest  84.36\ngaussian       82.68\nLinearSVC      82.68\nlogreg         82.12\nsgdclassifier  81.01\ndecision_tree  78.77\nperceptron     68.72",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>knn</th>\n      <td>85.47</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>84.36</td>\n    </tr>\n    <tr>\n      <th>random_forest</th>\n      <td>84.36</td>\n    </tr>\n    <tr>\n      <th>gaussian</th>\n      <td>82.68</td>\n    </tr>\n    <tr>\n      <th>LinearSVC</th>\n      <td>82.68</td>\n    </tr>\n    <tr>\n      <th>logreg</th>\n      <td>82.12</td>\n    </tr>\n    <tr>\n      <th>sgdclassifier</th>\n      <td>81.01</td>\n    </tr>\n    <tr>\n      <th>decision_tree</th>\n      <td>78.77</td>\n    </tr>\n    <tr>\n      <th>perceptron</th>\n      <td>68.72</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "models = [\"logreg\", \"SVC\", \"knn\", \"gaussian\", \"perceptron\", \"LinearSVC\", \"sgdclassifier\", \"decision_tree\", \"random_forest\"]\n",
    "\n",
    "scores = [82.12, 84.36, 85.47, 82.68, 68.72, 82.68, 81.01, 78.77, 84.36]\n",
    "\n",
    "df_scores_2 = pd.DataFrame(scores, index = models)\n",
    "df_scores_2 = df_scores_2. rename(columns={0:'Score'})\n",
    "df_scores_2.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "source": [
    "The overall general score did not increase. without the change the highest score was: Random Forest -\t86.76, now the highest score is knn with 85.47.  But the scores are higher then when using the pca. \n",
    "\n",
    "Also the best model without the changes and with pca was: random_forest, and with the normalization the best model is: knn. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "3. Añade, como columnas, la media, mediana, varianza y covarianza de todas las columnas numéricas no categóricas y comprueba si con esas nuevas columnas se aumenta el score de los algoritmos.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# following steps made:\n",
    "\n",
    "#train_df[\"mean_fare\"] = train_df.Fare.mean()\n",
    "#test_df[\"mean_fare\"] = test_df.Fare.mean()\n",
    "\n",
    "#train_df[\"median_fare\"] = train_df.Fare.median()\n",
    "#test_df[\"median_fare\"] = test_df.Fare.median()\n",
    "\n",
    "#train_df[\"variance_fare\"] = train_df.var()['Fare'] \n",
    "#test_df[\"variance_fare\"] = test_df.var()['Fare'] \n",
    "\n",
    "\n",
    "#--> to ask: .cov gives only nan values? So in this case I did not add the covariance. I tried it like this:\n",
    "#train_df[\"covariance_fare\"] = train_df.cov()['Fare'] \n",
    "#test_df[\"covariance_fare\"] = test_df.cov()['Fare']"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               Score\nrandom_forest  84.92\ngaussian       82.68\nlogreg         82.12\ndecision_tree  79.89\nknn            75.98\nLinearSVC      65.92\nSVC            65.36\nperceptron     37.99\nsgdclassifier  37.99",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>random_forest</th>\n      <td>84.92</td>\n    </tr>\n    <tr>\n      <th>gaussian</th>\n      <td>82.68</td>\n    </tr>\n    <tr>\n      <th>logreg</th>\n      <td>82.12</td>\n    </tr>\n    <tr>\n      <th>decision_tree</th>\n      <td>79.89</td>\n    </tr>\n    <tr>\n      <th>knn</th>\n      <td>75.98</td>\n    </tr>\n    <tr>\n      <th>LinearSVC</th>\n      <td>65.92</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>65.36</td>\n    </tr>\n    <tr>\n      <th>perceptron</th>\n      <td>37.99</td>\n    </tr>\n    <tr>\n      <th>sgdclassifier</th>\n      <td>37.99</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "models = [\"logreg\", \"SVC\", \"knn\", \"gaussian\", \"perceptron\", \"LinearSVC\", \"sgdclassifier\", \"decision_tree\", \"random_forest\"]\n",
    "\n",
    "scores = [82.12, 65.36, 75.98, 82.68, 37.99, 65.92, 37.99, 79.89, 84.92]\n",
    "\n",
    "\n",
    "df_scores_3 = pd.DataFrame(scores, index = models)\n",
    "df_scores_3 = df_scores_3. rename(columns={0:'Score'})\n",
    "df_scores_3.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "source": [
    "The score was not improved, the highest without changes was 86.76, the highest score with normalization was 85.47. So this has also not improved. The score, however, is better then the score with pca which was: 84.36."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "4. Realiza 1, 2 y 3 pero usando GridSearch para encontrar los mejores hiperparámetros (features) de los algoritmos de ML antes de sacar el score. Debes probar con al menos 3 features y 2 valores para cada feature como mínimo.\n",
    "\n",
    "- using gridsearch function imported from Utils.gridseach_model.py and if model was not there, made it and added it to function. \n",
    "- --> GaussianNB() does not have parameter tuning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "4.1. Realizar una reducción de dimensionalidad/PCA para intentar aumentar el porcentaje de acierto. ¿Aumenta? ¿Cómo cambia el acierto según diferentes valores de PCA? Prueba, como mínimo, con valores para \"n_components\" 2,4,5 y 0.45 y 0.95. ¿Ha habido algún problema con algún valor?\n",
    "\n",
    "Tried the following parameters for each n_component\n",
    "\n",
    "n_2:\n",
    "LogisticRegression(penalty='l1', solver='liblinear')\n",
    "SVC(C=1, kernel='linear', probability=True)\n",
    "KNeighborsClassifier(n_neighbors=9, weights='distance')\n",
    "gaurssian -> no parameters\n",
    "Perceptron(fit_intercept=False, shuffle=False)\n",
    "LinearSVC(C=1, loss='hinge')\n",
    "SGDClassifier(penalty='l1')\n",
    "DecisionTreeClassifier(criterion='entropy')\n",
    "RandomForestClassifier(criterion='entropy', n_estimators=50)\n",
    "\n",
    "n_4:\n",
    "LogisticRegression(penalty='none', solver='newton-cg')\n",
    "SVC(C=1, kernel='linear', probability=True)\n",
    "KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
    "gaurssian -> no parameters\n",
    "Perceptron(penalty='l1', shuffle=False)\n",
    "LinearSVC(C=8, loss='hinge')\n",
    "SGDClassifier(early_stopping=True, loss='log', penalty='elasticnet')\n",
    "DecisionTreeClassifier(min_samples_split=10, splitter='random')\n",
    "RandomForestClassifier(n_estimators=200, warm_start=True)\n",
    "\n",
    "n_5:\n",
    "LogisticRegression(penalty='l1', solver='liblinear')\n",
    "SVC(C=1, kernel='linear', probability=True)\n",
    "KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
    "gaurssian -> no parameters\n",
    "Perceptron(penalty='l2')\n",
    "LinearSVC(C=1, loss='hinge')\n",
    "SGDClassifier(loss='modified_huber', penalty='elasticnet')\n",
    "DecisionTreeClassifier(min_samples_split=5, splitter='random')\n",
    "RandomForestClassifier(criterion='entropy', max_features='sqrt',\n",
    "                       n_estimators=200)\n",
    "\n",
    "n_0.45:\n",
    "--> gave lots of error and froze system. So did not do the 0.45.\n",
    "\n",
    "\n",
    "n_0.95:\n",
    "LogisticRegression(penalty='l1', solver='liblinear')\n",
    "SVC(C=1, kernel='linear', probability=True)\n",
    "KNeighborsClassifier(n_neighbors=9, weights='distance')\n",
    "gaurssian -> no parameters\n",
    "Perceptron(fit_intercept=False, shuffle=False)\n",
    "LinearSVC(C=1, loss='hinge')\n",
    "SGDClassifier()\n",
    "DecisionTreeClassifier(criterion='entropy', min_samples_split=5,\n",
    "                       splitter='random')\n",
    "RandomForestClassifier(max_features='log2', n_estimators=200)\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               pca_2  pca_4  pca_5  pca_0.95\nmodel                                       \nrandom_forest  84.36  83.80  84.92     83.80\nSVC            75.42  83.24  82.12     75.42\nLinearSVC      75.42  83.24  82.12     75.42\ndecision_tree  82.12  83.24  82.68     81.56\nknn            82.68  82.68  82.68     82.68\nlogreg         73.74  81.01  80.45     73.74\nperceptron     46.37  72.63  71.51     46.37\ngaussian       36.31  67.04  69.27     36.31\nsgdclassifier  70.95  46.93  79.33     65.36",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pca_2</th>\n      <th>pca_4</th>\n      <th>pca_5</th>\n      <th>pca_0.95</th>\n    </tr>\n    <tr>\n      <th>model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>random_forest</th>\n      <td>84.36</td>\n      <td>83.80</td>\n      <td>84.92</td>\n      <td>83.80</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>75.42</td>\n      <td>83.24</td>\n      <td>82.12</td>\n      <td>75.42</td>\n    </tr>\n    <tr>\n      <th>LinearSVC</th>\n      <td>75.42</td>\n      <td>83.24</td>\n      <td>82.12</td>\n      <td>75.42</td>\n    </tr>\n    <tr>\n      <th>decision_tree</th>\n      <td>82.12</td>\n      <td>83.24</td>\n      <td>82.68</td>\n      <td>81.56</td>\n    </tr>\n    <tr>\n      <th>knn</th>\n      <td>82.68</td>\n      <td>82.68</td>\n      <td>82.68</td>\n      <td>82.68</td>\n    </tr>\n    <tr>\n      <th>logreg</th>\n      <td>73.74</td>\n      <td>81.01</td>\n      <td>80.45</td>\n      <td>73.74</td>\n    </tr>\n    <tr>\n      <th>perceptron</th>\n      <td>46.37</td>\n      <td>72.63</td>\n      <td>71.51</td>\n      <td>46.37</td>\n    </tr>\n    <tr>\n      <th>gaussian</th>\n      <td>36.31</td>\n      <td>67.04</td>\n      <td>69.27</td>\n      <td>36.31</td>\n    </tr>\n    <tr>\n      <th>sgdclassifier</th>\n      <td>70.95</td>\n      <td>46.93</td>\n      <td>79.33</td>\n      <td>65.36</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "\n",
    "models = [\"logreg\", \"SVC\", \"knn\", \"gaussian\", \"perceptron\", \"LinearSVC\", \"sgdclassifier\", \"decision_tree\", \"random_forest\"]\n",
    "\n",
    "data = {\"pca_2\":[73.74, 75.42, 82.68, 36.31, 46.37, 75.42, 70.95, 82.12, 84.36], \"pca_4\":[81.01, 83.24, 82.68, 67.04, 72.63, 83.24, 46.93, 83.24, 83.8], \"pca_5\": [80.45, 82.12, 82.68, 69.27, 71.51, 82.12, 79.33, 82.68, 84.92], \"pca_0.95\": [73.74, 75.42, 82.68, 36.31, 46.37, 75.42, 65.36, 81.56, 83.8]}\n",
    "\n",
    "df_scores_4 = pd.DataFrame(data, index = models)\n",
    "df_scores_4.index.name = 'model'\n",
    "\n",
    "df_scores_4.sort_values(by='pca_4', ascending=False)"
   ]
  },
  {
   "source": [
    "Now highest score is with: 84.92. Still lower than the original score, but slightly higher then the highest score using pca without the parameters.   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "4.2 Realizar una normalización de los datos no categóricos y comprobar si aumenta los scores.\n",
    "\n",
    "parameters used:\n",
    "\n",
    "LogisticRegression(solver='newton-cg')\n",
    "\n",
    "nan --> SVC kept on running without doing anything, froze system. \n",
    "\n",
    "KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "Perceptron(penalty='l1', shuffle=False)\n",
    "\n",
    "LinearSVC(C=1)\n",
    "\n",
    "SGDClassifier(loss='log', penalty='elasticnet')\n",
    "\n",
    "DecisionTreeClassifier(criterion='entropy', min_samples_split=10,\n",
    "                       splitter='random')\n",
    "                       \n",
    "RandomForestClassifier(warm_start=True)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               Score\nknn            87.15\nrandom_forest  84.92\ngaussian       82.68\nLinearSVC      82.68\nlogreg         82.12\ndecision_tree  82.12\nsgdclassifier  72.07\nperceptron     65.92\nSVC              NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>knn</th>\n      <td>87.15</td>\n    </tr>\n    <tr>\n      <th>random_forest</th>\n      <td>84.92</td>\n    </tr>\n    <tr>\n      <th>gaussian</th>\n      <td>82.68</td>\n    </tr>\n    <tr>\n      <th>LinearSVC</th>\n      <td>82.68</td>\n    </tr>\n    <tr>\n      <th>logreg</th>\n      <td>82.12</td>\n    </tr>\n    <tr>\n      <th>decision_tree</th>\n      <td>82.12</td>\n    </tr>\n    <tr>\n      <th>sgdclassifier</th>\n      <td>72.07</td>\n    </tr>\n    <tr>\n      <th>perceptron</th>\n      <td>65.92</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "models = [\"logreg\", \"SVC\", \"knn\", \"gaussian\", \"perceptron\", \"LinearSVC\", \"sgdclassifier\", \"decision_tree\", \"random_forest\"]\n",
    "\n",
    "scores = [82.12, None, 87.15, 82.68, 65.92, 82.68, 72.07, 82.12, 84.92]\n",
    "\n",
    "df_scores_5 = pd.DataFrame(scores, index = models)\n",
    "df_scores_5 = df_scores_5. rename(columns={0:'Score'})\n",
    "df_scores_5.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "source": [
    "Knn has now the a better score than the original model: 87.15"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "4.3 Añade, como columnas, la media, mediana, varianza y covarianza de todas las columnas numéricas no categóricas y comprueba si con esas nuevas columnas se aumenta el score de los algoritmos.\n",
    "\n",
    "parameters used:\n",
    "\n",
    "LogisticRegression(solver='newton-cg')\n",
    "\n",
    "2-> overgeslagen!\n",
    "\n",
    "KNeighborsClassifier(n_neighbors=11, weights='distance')\n",
    "\n",
    "Perceptron(shuffle=False)\n",
    "\n",
    "LinearSVC(C=1, loss='hinge')\n",
    "\n",
    "SGDClassifier(penalty='l1')\n",
    "\n",
    "DecisionTreeClassifier(min_samples_split=5, splitter='random')\n",
    "\n",
    "RandomForestClassifier(n_estimators=50, warm_start=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               Score\nrandom_forest  83.80\ngaussian       82.68\nlogreg         82.12\nSVC            81.56\nsgdclassifier  81.01\ndecision_tree  79.89\nknn            77.09\nperceptron     65.92\nLinearSVC      65.92",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>random_forest</th>\n      <td>83.80</td>\n    </tr>\n    <tr>\n      <th>gaussian</th>\n      <td>82.68</td>\n    </tr>\n    <tr>\n      <th>logreg</th>\n      <td>82.12</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>81.56</td>\n    </tr>\n    <tr>\n      <th>sgdclassifier</th>\n      <td>81.01</td>\n    </tr>\n    <tr>\n      <th>decision_tree</th>\n      <td>79.89</td>\n    </tr>\n    <tr>\n      <th>knn</th>\n      <td>77.09</td>\n    </tr>\n    <tr>\n      <th>perceptron</th>\n      <td>65.92</td>\n    </tr>\n    <tr>\n      <th>LinearSVC</th>\n      <td>65.92</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "models = [\"logreg\", \"SVC\", \"knn\", \"gaussian\", \"perceptron\", \"LinearSVC\", \"sgdclassifier\", \"decision_tree\", \"random_forest\"]\n",
    "\n",
    "scores = [82.12, 81.56, 77.09, 82.68, 65.92, 65.92, 81.01, 79.89, 83.8]\n",
    "\n",
    "\n",
    "df_scores_6 = pd.DataFrame(scores, index = models)\n",
    "df_scores_6 = df_scores_6. rename(columns={0:'Score'})\n",
    "df_scores_6.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "source": [
    "Highest score is:\t83.80 with random forest. This is not a new highest score. And also it is worse than when we did not use any parameters. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---------------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Conclusion: the highest score is when normalizing the fare column and then using: KNeighborsClassifier(n_neighbors=8)   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}